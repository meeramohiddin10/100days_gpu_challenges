{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXokyVndEMfX",
        "outputId": "7712d10f-575c-442f-e468-8caa2571f09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_add.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile vector_add.cu\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Function for parallel reduction in shared memory\n",
        "__device__ float reduceSum(volatile float* data, int size) {\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    // Reduce by summing elements\n",
        "    for (int s = size / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            data[tid] += data[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    return data[0];\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void LayerNorm(const float* A, float* B, int rows, int cols) {\n",
        "    // Calculate row index\n",
        "    int row = blockIdx.x; // Each block processes one row\n",
        "\n",
        "    if (row < rows) {\n",
        "        // Use shared memory for row-wise computation\n",
        "        extern __shared__ float shared[];\n",
        "        float* row_data = shared;\n",
        "        float* variance_data = shared; // Use the same shared memory for variance calculation\n",
        "\n",
        "        // Copy row data to shared memory\n",
        "        for (int col = threadIdx.x; col < cols; col += blockDim.x) {\n",
        "            row_data[col] = A[row * cols + col];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute mean using parallel reduction\n",
        "        float mean = 0.0f;\n",
        "        if (threadIdx.x < cols) { // Ensure threads don't go out of bounds for reduction\n",
        "           mean = reduceSum(row_data, cols);\n",
        "        }\n",
        "        __syncthreads(); // Synchronize after reduction\n",
        "\n",
        "        if (threadIdx.x == 0) {\n",
        "             mean /= cols; // Divide sum by cols to get the mean\n",
        "        }\n",
        "        __syncthreads(); // Synchronize after calculating mean\n",
        "\n",
        "        // Prepare data for variance calculation in shared memory\n",
        "        for (int col = threadIdx.x; col < cols; col += blockDim.x) {\n",
        "             variance_data[col] = (A[row * cols + col] - mean) * (A[row * cols + col] - mean); // Calculate squared difference using original data\n",
        "        }\n",
        "        __syncthreads(); // Synchronize after calculating squared differences\n",
        "\n",
        "        // Compute variance using parallel reduction\n",
        "        float variance = 0.0f;\n",
        "         if (threadIdx.x < cols) { // Ensure threads don't go out of bounds for reduction\n",
        "            variance = reduceSum(variance_data, cols);\n",
        "        }\n",
        "        __syncthreads(); // Synchronize after reduction\n",
        "\n",
        "        if (threadIdx.x == 0) {\n",
        "             variance /= cols; // Divide sum by cols to get the variance\n",
        "        }\n",
        "        __syncthreads(); // Synchronize after calculating variance\n",
        "\n",
        "\n",
        "        float stddev = sqrtf(variance + 1e-7);\n",
        "\n",
        "        // Normalize\n",
        "        for (int col = threadIdx.x; col < cols; col += blockDim.x) {\n",
        "            B[row * cols + col] = (A[row * cols + col] - mean) / stddev; // Use original A for normalization\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int rows = 10, cols = 10;\n",
        "    float *A, *B;\n",
        "\n",
        "    // Allocate host memory\n",
        "    A = (float*)malloc(rows * cols * sizeof(float));\n",
        "    B = (float*)malloc(rows * cols * sizeof(float));\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            A[i * cols + j] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_a, *d_b;\n",
        "    cudaMalloc(&d_a, rows * cols * sizeof(float));\n",
        "    cudaMalloc(&d_b, rows * cols * sizeof(float));\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_a, A, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    int blocksize = cols; // Use cols threads per block, one block per row\n",
        "    int gridsize = rows;\n",
        "    size_t shared_memory_size = cols * sizeof(float); // Allocate shared memory for one row\n",
        "    LayerNorm<<<gridsize, blocksize, shared_memory_size>>>(d_a, d_b, rows, cols);\n",
        "\n",
        "    // Synchronize device\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(B, d_b, rows * cols * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    printf(\"A:\\n\");\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            printf(\"%.2f \", A[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nB:\\n\");\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            printf(\"%.2f \", B[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    free(A);\n",
        "    free(B);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc vector_add.cu -o vector_add -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./vector_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hVTjUK4EXPx",
        "outputId": "059f5fcd-6374-4488-eaa2-edf1f983d918"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            "0.84 0.39 0.78 0.80 0.91 0.20 0.34 0.77 0.28 0.55 \n",
            "0.48 0.63 0.36 0.51 0.95 0.92 0.64 0.72 0.14 0.61 \n",
            "0.02 0.24 0.14 0.80 0.16 0.40 0.13 0.11 1.00 0.22 \n",
            "0.51 0.84 0.61 0.30 0.64 0.52 0.49 0.97 0.29 0.77 \n",
            "0.53 0.77 0.40 0.89 0.28 0.35 0.81 0.92 0.07 0.95 \n",
            "0.53 0.09 0.19 0.66 0.89 0.35 0.06 0.02 0.46 0.06 \n",
            "0.24 0.97 0.90 0.85 0.27 0.54 0.38 0.76 0.51 0.67 \n",
            "0.53 0.04 0.44 0.93 0.93 0.72 0.28 0.74 0.64 0.35 \n",
            "0.69 0.17 0.44 0.88 0.83 0.33 0.23 0.89 0.35 0.69 \n",
            "0.96 0.59 0.66 0.86 0.44 0.92 0.40 0.81 0.68 0.91 \n",
            "\n",
            "B:\n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n"
          ]
        }
      ]
    }
  ]
}